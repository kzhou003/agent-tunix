# AdamW optimizer configuration
learning_rate: 3e-6
beta1: 0.9
beta2: 0.99
weight_decay: 0.1
warmup_ratio: 0.1
max_grad_norm: 0.1
