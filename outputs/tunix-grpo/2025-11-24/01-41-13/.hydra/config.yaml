model:
  model_family: gemma3
  model_size: 270m
  lora_rank: 4
  lora_alpha: 8.0
  lora_module_path: .*q_einsum
  mesh_shape:
  - - 1
  - - fsdp
optimizer:
  learning_rate: 3.0e-06
  beta1: 0.9
  beta2: 0.99
  weight_decay: 0.1
  warmup_ratio: 0.1
  max_grad_norm: 0.1
scheduler:
  warmup_steps_ratio: 0.1
  decay_steps_ratio: 1.0
grpo:
  num_generations: 2
  num_iterations: 1
  beta: 0.08
  epsilon: 0.2
generation:
  max_prompt_length: 64
  max_generation_steps: 64
  temperature: 0.9
  top_p: 1.0
  top_k: 50
  eos_tokens:
  - 1
  - 106
training:
  train_data_dir: ./data/train
  test_data_dir: ./data/test
  data_source: huggingface
  train_fraction: 1.0
  micro_batch_size: 1
  num_batches: 10
  num_test_batches: 5
  num_epochs: 1
  eval_every_n_steps: 10
  checkpoint_dir: ./checkpoints/ckpts/
  intermediate_checkpoint_dir: ./checkpoints/intermediate/
  save_interval_steps: 500
  max_checkpoints_to_keep: 4
  log_dir: ./checkpoints/tensorboard/
  flush_every_n_steps: 20
  wandb_project: tunix-grpo
project_name: tunix-grpo
seed: 42
debug: true
wandb_disabled: true
experiment_name: quick_test
tags:
- testing
- quick
- reduced
