model:
  model_family: gemma3
  model_size: 270m
  lora_rank: 32
  lora_alpha: 32.0
  lora_module_path: .*q_einsum|.*kv_einsum|.*gate_proj|.*down_proj|.*up_proj|.*attn_vec_einsum
  mesh_shape:
  - - 1
    - 4
  - - fsdp
    - tp
optimizer:
  learning_rate: 3.0e-06
  beta1: 0.9
  beta2: 0.99
  weight_decay: 0.1
  warmup_ratio: 0.1
  max_grad_norm: 0.1
scheduler:
  warmup_steps_ratio: 0.1
  decay_steps_ratio: 1.0
grpo:
  num_generations: 4
  num_iterations: 1
  beta: 0.08
  epsilon: 0.2
generation:
  max_prompt_length: 256
  max_generation_steps: 512
  temperature: 0.9
  top_p: 1.0
  top_k: 50
  eos_tokens:
  - 1
  - 106
training:
  train_data_dir: ./data/train
  test_data_dir: ./data/test
  data_source: huggingface
  train_fraction: 1.0
  micro_batch_size: 4
  num_batches: 3738
  num_test_batches: 100
  num_epochs: 1
  eval_every_n_steps: 10
  checkpoint_dir: ./checkpoints/ckpts/
  intermediate_checkpoint_dir: ./checkpoints/intermediate/
  save_interval_steps: 500
  max_checkpoints_to_keep: 4
  log_dir: ./checkpoints/tensorboard/
  flush_every_n_steps: 20
  wandb_project: tunix-grpo
project_name: tunix-grpo
seed: 42
debug: false
